---
slug: "ai-journey-northwestern"
title: "From Sound to Signal: My AI Journey Beginning in Northwestern's Music & Audio Lab"
date: 2024-10-12
author: Said Yaka
tags:
  - AI
  - Northwestern
  - Machine Learning
  - Music Technology
---

![Northwestern University Campus](/images/northwestern-campus.jpg)

My fascination with artificial intelligence didn't begin in a traditional computer science lecture—it started in a classroom alive with sound. At Northwestern University, I enrolled in COMP_SCI 352: Machine Perception of Music & Audio, a course that changed how I perceive both music and computation.

In that class, we dove into how a computer can hear and understand music. We studied ways to extract musical structure from raw audio, MIDI, and score files—learning about source separation, perceptual mappings, and the algorithms that let machines quantify sound. The course pressed us to build tools that detect salient musical features, to think critically about the linkage between auditory perception and data, and to engage with state-of-the-art research in music information retrieval.

Before taking 352, I had experience in programming, but this was a new frontier: not just processing data, but processing art. As we worked on assignments, I found myself debugging neural networks that tried to disentangle overlapping instruments, or mapping rhythmic patterns to expressive representations. The idea that we could teach a machine to "listen" was thrilling.

But the course was more than just algorithms and signal processing theory. Its objectives included understanding how humans perceive sound, and then paralleling that in computational models. We were pushed not just to build, but to reflect: What does it mean for a machine to notice what matters in a song? And how do we bridge the gap between human intuition and quantifiable structure?

This deep musical-technical integration cemented my belief that AI doesn't have to be confined to vision or language—it can live in sound, in creativity, in emotion. From that point, I began exploring how generative models might assist composers, how style transfer could cross modalities, and how hybrid systems might fuse human spontaneity with machine consistency.

As I progressed in my AI work, I carried those lessons forward. The framework I learned in 352—extract structure, map patterns, design responsive feedback systems—became applicable beyond music: in natural language tasks, in generative art, in real-time interaction systems.

When I imagine the origin of my passion for AI, I see that Northwestern lab space filled with headphones, sound boards, oscillators, and code. That's where I first truly saw AI as a collaborator, not just a tool. And it's what still motivates me: building systems that don't simply compute, but listen, respond, and co-create.
